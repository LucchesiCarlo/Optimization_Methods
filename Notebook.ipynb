{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1eaed66-8edb-461c-9330-590b35f8ee88",
   "metadata": {},
   "source": [
    "# Optimization Methods Project Work: SMO and DCD-Linear for training SVM\n",
    "\n",
    "This notebook will contain some code that implements the SMO and DCD-Linear algorithm, plus the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd80a2-121b-44ba-bb29-7f984b1fd694",
   "metadata": {},
   "source": [
    "## Testing the environment\n",
    "\n",
    "The following cell will simply verify if all the libraries necessary to run the code are correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e951473-fb90-43f0-a46f-768080d5fbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is good!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(91212) # This function allows to replicate the same experiemtns even if the random function is used\n",
    "\n",
    "print(\"Everything is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575adfd-ebce-48a2-b354-cce48c8f4d22",
   "metadata": {},
   "source": [
    "## Defining the Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a82c1-39da-495f-b77d-788f67e5cc1a",
   "metadata": {},
   "source": [
    "### The Bias One (For SMO)\n",
    "\n",
    "As we know, the formulation of the dual problem for training a SVM is:\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\min_{\\alpha} \\frac{1}{2}\\alpha^TQ\\alpha - e^T\\alpha \\\\\n",
    "\\forall i\\ 0 \\leq \\alpha_i \\leq C\\ \\ \\sum \\limits_i^n \\alpha_iy_i = 0\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "As stated in the Project Goals, we will use the Most Violating Pair rule to select the variables to change. So, to implement this algorithm efficiently, we need those elements:\n",
    "* A function that calculates the derivatives\n",
    "* A function that adjust the derivative after changing $\\alpha$ \n",
    "* A function that extract the most violating pair.\n",
    "\n",
    "To sum up all this functionality, they will be implemented inside a class. The constructor will receive the set of Xs and Ys, and using that will derive Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "66e08b30-7578-4d4d-92a3-5ceafa96e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualSVMProblem:\n",
    "\n",
    "    def __init__(self, Xs, Ys, C, epsilon = 10e-4):\n",
    "        self.X = Xs\n",
    "        self.Y = Ys\n",
    "        self.C = C\n",
    "        self.a = np.zeros_like(Ys, dtype = float)\n",
    "        self.e = np.ones_like(self.a)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        mat = np.stack([Ys.T] * Xs.shape[1], axis = 1)\n",
    "        \n",
    "        Z = Xs * mat\n",
    "        self.Q = Z @ Z.T\n",
    "\n",
    "        self.d = self.Q @ self.a - self.e\n",
    "\n",
    "    def getA(self):\n",
    "        return self.a\n",
    "        \n",
    "    def getDerivative(self):\n",
    "        return self.d    \n",
    "\n",
    "    def getMostViolatingPair(self):\n",
    "        directions = self.d / self.Y\n",
    "        min_idx = -1\n",
    "        min_value = np.inf\n",
    "        max_idx = -1\n",
    "        max_value = -np.inf\n",
    "\n",
    "        R = directions.copy()\n",
    "        R[np.logical_and(self.a < self.epsilon, self.Y == -1)] = np.inf\n",
    "        R[np.logical_and(self.a > self.C - self.epsilon, self.Y == 1)] = np.inf\n",
    "\n",
    "        S = directions.copy()\n",
    "        S[np.logical_and(self.a < self.epsilon, self.Y == 1)] = -np.inf\n",
    "        S[np.logical_and(self.a > self.C - self.epsilon, self.Y == -1)] = -np.inf\n",
    "\n",
    "        min_idx = np.argmin(R)\n",
    "        max_idx = np.argmax(S)\n",
    "\n",
    "        if R[min_idx] == np.inf or S[max_idx] == -np.inf:\n",
    "            return None\n",
    "                \n",
    "        return (min_idx, max_idx)\n",
    "\n",
    "    def updateA(self, idx1, a1, idx2, a2):\n",
    "        self.d = self.Q[idx1] * (a1 - self.a[idx1]) + self.Q[idx2] * (a2 - self.a[idx2]) + self.d\n",
    "        self.a[idx1] = a1\n",
    "        self.a[idx2] = a2\n",
    "\n",
    "    def getParameters(self):\n",
    "        mat = np.stack([self.a.T * self.Y.T] * self.X.shape[1], axis = 1)\n",
    "        w = np.sum(mat * self.X, axis = 0)\n",
    "        b = 0\n",
    "        for i in range(len(self.a)):\n",
    "            if(self.a[i] < self.epsilon or self.a[i] > self.C - self.epsilon):\n",
    "                continue\n",
    "            b = 1 / self.Y[i] - w @ self.X[i]\n",
    "            break        \n",
    "        return (w, b)\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.sign(x @ self.w + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "98bd03a6-1ec2-4f40-9708-ff03ab0c7847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is working as expected!\n"
     ]
    }
   ],
   "source": [
    "# Let's test the code with some artificial data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([1, -1])\n",
    "C = 5\n",
    "\n",
    "# The safe way to calculate matrix Q\n",
    "Q = np.zeros((2, 2))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        Q[i][j] = Y[i] * Y[j] * X[i] @ X[j]\n",
    "\n",
    "problem = DualSVMProblem(X, Y, C)\n",
    "\n",
    "ok = True\n",
    "if ((Q != problem.Q).all()):\n",
    "    ok = False\n",
    "    print(\"The matix calculation is wrong\")\n",
    "    \n",
    "if ((np.array([-1, -1]) != problem.getDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative function if wrong\")\n",
    "\n",
    "update = np.array([1, 2])\n",
    "problem.updateA(0, update[0], 1, update[1])\n",
    "\n",
    "if ((Q @ update - np.ones(2) != problem.getDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The updates of variables doesn't update the derivative in the right way\")\n",
    "\n",
    "elements = problem.getDerivative() / Y\n",
    "\n",
    "if not (problem.getMostViolatingPair()[0] == (0 if elements[0] < elements[1] else 1) and problem.getMostViolatingPair()[1] == (0 if elements[0] > elements[1] else 1)):\n",
    "    ok = False\n",
    "    print(\"The calculation of the most violating pair is wrong\")\n",
    "    # This test is a bit odd and not exaustive, but can help\n",
    "\n",
    "if ok:\n",
    "    print(\"Everything is working as expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7369272-b06d-47ea-9f7c-490ee7b7b4c6",
   "metadata": {},
   "source": [
    "## Implementation of SMO\n",
    "\n",
    "After implementing core elements for the SMO algorithm, it's time to get them together and build the actual result.\n",
    "In this implementation, I realized a function that execute a single iteration of the algorithm, leaving the check of convergence outside. The objective is to reuse the external function with the different approach (DCD-Linear), and then make easier to instrument the code to measure some components like loss, validation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d3b9a1bb-e679-46c6-8407-2b7170ade1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def trainingSMO(problem: DualSVMProblem, step: Callable[[DualSVMProblem], bool], loss_func: Callable[[SVM], float] = None, validation_func: Callable[[SVM], float] = None, epsilon = 10e-5):\n",
    "    loss = []\n",
    "    validation = []\n",
    "    \n",
    "    while(True): \n",
    "        (min_idx, max_idx) = problem.getMostViolatingPair()\n",
    "        elements = problem.getDerivative() / problem.Y\n",
    "\n",
    "        if(elements[min_idx] + epsilon > elements[max_idx]):\n",
    "            break\n",
    "        \n",
    "        result = step(problem)            \n",
    "        (w, b) = problem.getParameters()\n",
    "        svm = SVM(w, b)\n",
    "        \n",
    "        if(loss_func is not None):\n",
    "            loss.append(loss_func(svm))\n",
    "    \n",
    "        if(validation_func is not None):\n",
    "            validation.append(validation_func(svm))\n",
    "\n",
    "        if not result:\n",
    "            # If the step method give False as a result indicates an error or a stop condition (i.e. having a derivative under the tollerance)\n",
    "            break\n",
    "\n",
    "    return (loss, validation)\n",
    "\n",
    "def SMO_step(problem: DualSVMProblem, epsilon: float = 10e-5):\n",
    "    test = problem.getMostViolatingPair()\n",
    "    if(test is None):\n",
    "        raise Exception(\"The problem did not give a valid violating pair\")\n",
    "    (min_idx, max_idx) = (test[0], test[1])\n",
    "\n",
    "    direction = np.zeros_like(problem.a)\n",
    "    direction[min_idx] = 1 * problem.Y[min_idx]\n",
    "    direction[max_idx] = - 1 * problem.Y[max_idx] #In theory I should divide, but y is in {-1, 1} making the multiplication equivalent, but more efficient\n",
    "\n",
    "    derivative = problem.getDerivative()\n",
    "\n",
    "    b = problem.C - problem.a[min_idx] if direction[min_idx] > 0 else problem.a[min_idx]\n",
    "    b = min(b, problem.C - problem.a[max_idx] if direction[max_idx] > 0 else problem.a[max_idx])\n",
    "    \n",
    "    if b < epsilon:\n",
    "        return False\n",
    "\n",
    "    value = direction.T @ problem.Q @ direction\n",
    "\n",
    "    if value > epsilon:\n",
    "        b = min(b, -(derivative.T @ direction) / value)\n",
    "\n",
    "    problem.updateA(min_idx, problem.a[min_idx] + b * direction[min_idx], max_idx, problem.a[max_idx] + b * direction[max_idx])\n",
    "    return True    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfe3fd-1d40-498b-86d3-981c76c60526",
   "metadata": {},
   "source": [
    "## Testing the Code\n",
    "\n",
    "In the following cells, I will use a very simple classification problem to see if the code works at all. [This dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#breast-cancer) is composed by only 683 elements, everyone of only 10 features. This dataset is chosen dues to it contained dimension. In fact, this implementation of the Dual SVM explicitly calculate the Q matrix, something that can become prohibitive very quickly in the increase of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8f51036a-2699-43c4-8c23-cfddcb56838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.860107, -0.111111, -1.      , ..., -0.555556, -1.      ,\n",
       "        -1.      ],\n",
       "       [-0.859671, -0.111111, -0.333333, ..., -0.555556, -0.777778,\n",
       "        -1.      ],\n",
       "       [-0.857807, -0.555556, -1.      , ..., -0.555556, -1.      ,\n",
       "        -1.      ],\n",
       "       ...,\n",
       "       [-0.876716, -0.111111,  1.      , ...,  0.555556,  1.      ,\n",
       "        -0.777778],\n",
       "       [-0.875424, -0.333333,  0.555556, ...,  1.      ,  0.111111,\n",
       "        -1.      ],\n",
       "       [-0.875424, -0.333333,  0.555556, ...,  1.      , -0.333333,\n",
       "        -1.      ]], shape=(683, 10))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(Xs, Ys) = load_svmlight_file(\"breast_cancer_scale\")\n",
    "Ys[Ys == 2] = 1\n",
    "Ys[Ys == 4] = -1 # TODO control the description to decide what is positive and what is negative\n",
    "\n",
    "Xs = Xs.toarray()\n",
    "\n",
    "Xs \n",
    "\n",
    "# There is a lot to say (Xs originally is a sparce matrix, not scaled values broke the model, Ys classes are 2 and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2527adb5-0227-4932-b206-5e3ca3be7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report on train set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.95      0.96      0.95       181\n",
      "         1.0       0.98      0.98      0.98       365\n",
      "\n",
      "    accuracy                           0.97       546\n",
      "   macro avg       0.96      0.97      0.96       546\n",
      "weighted avg       0.97      0.97      0.97       546\n",
      "\n",
      "Report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.98      0.98      0.98        58\n",
      "         1.0       0.99      0.99      0.99        79\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.99      0.99      0.99       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(train_Xs, test_Xs, train_Ys, test_Ys)  = train_test_split(Xs, Ys, test_size = 0.20)\n",
    "\n",
    "base_svm = LinearSVC(C = 10)\n",
    "base_svm.fit(train_Xs, train_Ys)\n",
    "\n",
    "train_preds = base_svm.predict(train_Xs)\n",
    "test_preds = base_svm.predict(test_Xs)\n",
    "\n",
    "\n",
    "print(\"Report on train set:\")\n",
    "print(classification_report(train_Ys, train_preds, zero_division = 0))\n",
    "\n",
    "print(\"Report on test set:\")\n",
    "print(classification_report(test_Ys, test_preds, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c9ff7ffa-a036-4b5c-b3b8-84e12eb643c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the model trained using SMO on training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.95      0.97      0.96       181\n",
      "         1.0       0.98      0.97      0.98       365\n",
      "\n",
      "    accuracy                           0.97       546\n",
      "   macro avg       0.96      0.97      0.97       546\n",
      "weighted avg       0.97      0.97      0.97       546\n",
      "\n",
      "Result of the model trained using SMO on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.98      0.98      0.98        58\n",
      "         1.0       0.99      0.99      0.99        79\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.99      0.99      0.99       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_smo_problem = DualSVMProblem(train_Xs, train_Ys, C = 10)\n",
    "\n",
    "(_, _) = trainingSMO(base_smo_problem, SMO_step, epsilon = 10e-5)\n",
    "\n",
    "(w, b) = base_smo_problem.getParameters()\n",
    "smo_svm = SVM(w, b)\n",
    "smo_train_preds = smo_svm(train_Xs)\n",
    "smo_test_preds = smo_svm(test_Xs)\n",
    "\n",
    "print(\"Result of the model trained using SMO on training set:\")\n",
    "print(classification_report(train_Ys, smo_train_preds, zero_division=0))\n",
    "\n",
    "print(\"Result of the model trained using SMO on test set:\")\n",
    "print(classification_report(test_Ys, smo_test_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "13d003af-5138-4bef-9e55-a9d98904a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in norm of w: 1.2787247615522017\n",
      "\"Correlation\" between the direction of ws : 0.9292058864991288\n",
      "Difference in the value b: [1.494006]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "print(f\"Difference in norm of w: {norm(w - base_svm.coef_)}\")\n",
    "print(f'\"Correlation\" between the direction of ws : {(w.T @ base_svm.coef_[0]) / (norm(w) * norm(base_svm.coef_))}')\n",
    "print(f\"Difference in the value b: {abs(b - base_svm.intercept_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b86a08-a5b2-44b8-8e68-b8ec942034e1",
   "metadata": {},
   "source": [
    "## DCD Linear\n",
    "\n",
    "After implementing the SMO algorithm, now it's time to implement the DCD version. To maintain the uniformity in the problem definition, I will implement the dual formulation of the L1-SVM problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ba695671-5aa8-4deb-892a-081b7671c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDDualProblem:\n",
    "\n",
    "    def __init__(self, Xs, Ys, C, epsilon = 10e-5):\n",
    "        self.X = Xs\n",
    "        self.Y = Ys\n",
    "        self.C = C\n",
    "        self.a = np.zeros_like(Ys, dtype = float)\n",
    "        self.e = np.ones_like(Ys)\n",
    "        self.diagQ = np.pow(np.sum(np.pow(Xs, 2), axis = 1, dtype = float), -1) #Inverse Diagonal of Q\n",
    "\n",
    "        self.w = np.zeros(Xs.shape[1], dtype = float)\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def optimizeOver(self, index):\n",
    "        G = self.Y[index] * self.w.T @ self.X[index] - 1\n",
    "\n",
    "        if self.a[index] < self.epsilon:\n",
    "            PG = min(G, 0)\n",
    "        elif self.a[index] > self.C - self.epsilon:\n",
    "            PG = max(G, 0)\n",
    "        else:\n",
    "            PG = G\n",
    "\n",
    "        if abs(PG) < self.epsilon:\n",
    "            return\n",
    "\n",
    "        new_a = min(max(self.a[index] - G * self.diagQ[index], 0), self.C) #Notice that during initialization of the class diagQ is already inverted. In this way the code should be more efficient\n",
    "        self.w = self.w + (new_a - self.a[index]) * self.Y[index] * self.X[index]\n",
    "        self.a[index] = new_a\n",
    "\n",
    "    def getProjectedDerivative(self):\n",
    "        derivative = self.Y * (self.X @ self.w) - self.e\n",
    "        derivative[np.logical_and(self.a < self.epsilon, derivative > -self.epsilon)] = 0\n",
    "        derivative[np.logical_and(self.a > C - self.epsilon, derivative < self.epsilon)] = 0\n",
    "        return derivative\n",
    "\n",
    "    def getParameters(self):\n",
    "        return self.w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a3e4-b2e8-4a48-930a-15acd700d674",
   "metadata": {},
   "source": [
    "### Performing some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6bf2accf-e0ef-47c3-a5be-3d056cabf92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is working as expected!\n"
     ]
    }
   ],
   "source": [
    "# Let's test the code with some artificial data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([1, -1])\n",
    "C = 5\n",
    "\n",
    "diagQ = np.array([1 / (X[0].T @ X[0]), 1 / (X[1].T @ X[1])])\n",
    "\n",
    "problem = DCDDualProblem(X, Y, C)\n",
    "\n",
    "ok = True\n",
    "if (not (diagQ == problem.diagQ).all()):\n",
    "    ok = False\n",
    "    print(\"The diagonal of the matix is wrong\")\n",
    "\n",
    "if (not (-np.ones(2) == problem.getProjectedDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (in the initial state)\")\n",
    "\n",
    "problem.optimizeOver(0)\n",
    "\n",
    "if (not (np.array([1 / 14, 0]) == problem.a).all()):\n",
    "    ok = False\n",
    "    print(\"The optimization function doesn't work.\")\n",
    "\n",
    "derivative = np.array([Y[i] * problem.w.T @ X[i] - 1 for i in range(X.shape[0])])\n",
    "if (not (derivative == problem.getProjectedDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (after update)\")\n",
    "\n",
    "if (norm(np.array([1, 2, 3]) / 14 - problem.w) > 10e-5):\n",
    "    ok = False\n",
    "    print(f\"The w is wrong {problem.w}\")\n",
    "\n",
    "### Tests on second update\n",
    "problem.optimizeOver(1)\n",
    "if (norm(np.array([1, 46 / 77]) / 14 - problem.a) > 10e-5):\n",
    "    ok = False\n",
    "    print(\"The optimization function doesn't work (second update).\")\n",
    "\n",
    "derivative = np.array([Y[i] * problem.w.T @ X[i] - 1 for i in range(X.shape[0])])\n",
    "if (norm(derivative - problem.getProjectedDerivative()) > 10e-5):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (second update)\")\n",
    "\n",
    "if ok:\n",
    "    print(\"Everything is working as expected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741f2a8-320c-4f30-b25f-9a76f1024fa9",
   "metadata": {},
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5714d161-2009-4013-8bc9-37baaee82df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingDCD(problem: DCDDualProblem, loss_func: Callable[[SVM], float] = None, validation_func: Callable[[SVM], float] = None, epsilon = 10e-5):\n",
    "    loss = []\n",
    "    validation = []\n",
    "    j = 0\n",
    "    while(norm(problem.getProjectedDerivative()) > epsilon):\n",
    "        j+=1\n",
    "        for i in np.random.permutation(len(problem.a)):\n",
    "            problem.optimizeOver(i)\n",
    "            w = problem.getParameters()\n",
    "            svm = SVM(w, 0)\n",
    "            \n",
    "            if(loss_func is not None):\n",
    "                loss.append(loss_func(svm))\n",
    "        \n",
    "            if(validation_func is not None):\n",
    "                validation.append(validation_func(svm))\n",
    "                \n",
    "    return (loss, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "dd2c497f-bb3e-478b-a28f-4e2a81842aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_bias_train = np.atleast_2d(np.ones(train_Xs.shape[0])).T\n",
    "explicit_bias_test = np.atleast_2d(np.ones(test_Xs.shape[0])).T\n",
    "\n",
    "train_Xs_unbiased = np.hstack([train_Xs, explicit_bias_train])\n",
    "test_Xs_unbiased = np.hstack([test_Xs, explicit_bias_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fa503204-d65f-4476-982e-7870a2410802",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dcd_problem = DCDDualProblem(train_Xs_unbiased, train_Ys, C = 5)\n",
    "\n",
    "(_, _) = trainingDCD(base_dcd_problem, epsilon = 10e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e290bb0b-3b8d-4cea-8cb9-33b5acbf09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the model trained using SMO on training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.95      0.96      0.95       181\n",
      "         1.0       0.98      0.97      0.98       365\n",
      "\n",
      "    accuracy                           0.97       546\n",
      "   macro avg       0.96      0.96      0.96       546\n",
      "weighted avg       0.97      0.97      0.97       546\n",
      "\n",
      "Result of the model trained using SMO on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.98      0.98      0.98        58\n",
      "         1.0       0.99      0.99      0.99        79\n",
      "\n",
      "    accuracy                           0.99       137\n",
      "   macro avg       0.99      0.99      0.99       137\n",
      "weighted avg       0.99      0.99      0.99       137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = base_dcd_problem.getParameters()\n",
    "\n",
    "dcd_svm = SVM(w, 0)\n",
    "dcd_train_preds = dcd_svm(train_Xs_unbiased)\n",
    "dcd_test_preds = dcd_svm(test_Xs_unbiased)\n",
    "\n",
    "print(\"Result of the model trained using SMO on training set:\")\n",
    "print(classification_report(train_Ys, dcd_train_preds, zero_division=0))\n",
    "\n",
    "print(\"Result of the model trained using SMO on test set:\")\n",
    "print(classification_report(test_Ys, dcd_test_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66116284-3148-4053-9f66-117fc38d2b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
