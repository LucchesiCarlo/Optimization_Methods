{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1eaed66-8edb-461c-9330-590b35f8ee88",
   "metadata": {},
   "source": [
    "# Optimization Methods Project Work: SMO and DCD-Linear for training SVM\n",
    "\n",
    "This notebook will contain the code used to accomplish the Laboratory of Optimization Methods.\n",
    "\n",
    "The goal of it is to compare the performance between two algorithms for training linear SVM: Sequential Minimal Optimization (SMO), ad Dual Coordinates Decent (DCD). In this Notebook these algorithms will be implemented, and compared with the `LinearSVC` of `scikit-learn` to assert the correctness. After that, there will be comparative tests on performance over different settings.\n",
    "\n",
    "## How are they different? And why are we testing them? \n",
    "In the context of SVM training, one of the biggest advantage of using the dual formulation comes from the **Kernel Trick**, that allows to implicitly map data on higher dimensions. In cases where data already has a lot of features, using the Kernel Trick is less effective, or even counterproductive. This thing said, **Linear SVM** are preferred, giving the possibility a more efficient formulation.\n",
    "\n",
    "So, be $X \\in \\mathbf{R}^{n\\times p}$ the dataset of $n$ elements and $p$ features, and $Y \\in \\{-1, 1\\}^n$ the corresponding classes.\n",
    "### SMO\n",
    "SMO is a common way to solve the dual formulation of SVM training loss, and can be used even with the Kernel Trick.\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\min_\\alpha\\frac{1}{2}\\alpha^TQ\\alpha - e^T\\alpha\\\\\n",
    "\\alpha^Ty = 0\\\\\n",
    "0 \\leq \\alpha \\leq C\\\\\n",
    "\\end{gathered}\n",
    "$$\n",
    "This problem has a variable for every element in the dataset, making convenient the use of decomposition methods. But, due to the constraint $\\alpha^Ty = 0$, we need to change 2 variables at time. It takes some elaboration to find a couple that gives an admissible and descent direction. Concluding, the overall cost to a single iteration is $O(pn)$.\n",
    "\n",
    "### DCD\n",
    "If we limit the use of a linear kernel, we can formulate the primal problem without a bias. To regain the same expressivity, we will add a constant feature equal to 1 for every element.\n",
    "$$\n",
    "\\min_w \\frac{1}{2}||w||^2 +C\\sum_i^n\\max\\{0, 1 -y^i(w^Tx^i)\\}\n",
    "$$\n",
    "This formulation in the dual becomes the same, but without the equality constraint.\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\min_\\alpha\\frac{1}{2}\\alpha^TQ\\alpha - e^T\\alpha\\\\\n",
    "0 \\leq \\alpha \\leq C\\\\\n",
    "\\end{gathered}\n",
    "$$\n",
    "This simplifies the cost of a single step, that can be performed a variable at time. The overall cost per step is $O(p)$, but requiring more steps than SMO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd80a2-121b-44ba-bb29-7f984b1fd694",
   "metadata": {},
   "source": [
    "## Testing the environment\n",
    "\n",
    "The following cell will simply verify if all the libraries necessary to run the code are correctly installed. This cell **must** be executed every time, because contains every import used during all the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dce27a1-20d8-49cf-92da-c5269717e633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is installed correctly!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "from numpy.linalg import norm\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable\n",
    "\n",
    "print(\"Everything is installed correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa005e3-46c9-4a95-82a1-43122be79b67",
   "metadata": {},
   "source": [
    "## Some support function\n",
    "\n",
    "In this cell are defined some of the function that helps during all the experiments. There is also a definition of the SVM class, that can be called as a function and operate in the same way the classifies of `scikit-learn` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c47c53b-8040-42cb-8e26-95a57f99f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str, dimension: int = None, features: list = None):\n",
    "    bias = load_svmlight_file(name)\n",
    "    bias = (bias[0].toarray(), bias[1])\n",
    "\n",
    "    if(dimension is not None):\n",
    "        permutation = np.random.permutation(bias[0].shape[0])\n",
    "\n",
    "        indexs = permutation[0:dimension]\n",
    "        bias = (bias[0][indexs], bias[1][indexs])\n",
    "\n",
    "    if features is not None:\n",
    "        bias = (bias[0][:, features], bias[1])\n",
    "    \n",
    "    new_column = np.atleast_2d(np.ones(bias[0].shape[0])).T\n",
    "    unbias = (np.hstack([bias[0], new_column]), bias[1])\n",
    "\n",
    "    return (bias, unbias)\n",
    "    \n",
    "def feature_extractor(feature_num: int, original_num: int):\n",
    "    permutation = np.random.permutation(original_num)\n",
    "    return np.sort(permutation[0:feature_num])\n",
    "    \n",
    "def calculate_accuracy(true_y, data, model):\n",
    "    preds = model(data)\n",
    "    return classification_report(true_y, preds, zero_division=0, output_dict=True)[\"accuracy\"]\n",
    "            \n",
    "class SVM:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.sign(x @ self.w + self.b)\n",
    "\n",
    "def dual_loss_svm(a, Q):\n",
    "    return 0.5 * (a.T @ Q @ a) - np.sum(a)\n",
    "\n",
    "def loss_svm_obj(svm, C, X, y):\n",
    "    return loss_svm(svm.w, svm.b, C, X, y)\n",
    "    \n",
    "def loss_svm(w, b, C, X, y):\n",
    "    psi = np.maximum(np.zeros_like(y), np.ones_like(y) - y * (X @ w + np.ones_like(y) * b))\n",
    "    return 0.5 * (w.T @ w) + C * np.sum(psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a82c1-39da-495f-b77d-788f67e5cc1a",
   "metadata": {},
   "source": [
    "## Implementing SMO utilities\n",
    "\n",
    "As we know, the formulation of the dual problem for training a SVM is:\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\min_{\\alpha} \\frac{1}{2}\\alpha^TQ\\alpha - e^T\\alpha \\\\\n",
    "\\forall i\\ 0 \\leq \\alpha_i \\leq C\\ \\ \\sum \\limits_i^n \\alpha_iy_i = 0\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "As stated in the Project Goals, we will use the Most Violating Pair rule to select the variables to change. So, to implement this algorithm efficiently, we need those elements:\n",
    "* A function that calculates the derivatives\n",
    "* A function that adjust the derivative after changing $\\alpha$ \n",
    "* A function that extract the most violating pair.\n",
    "\n",
    "To sum up all this functionality, they will be implemented inside a class. The constructor will receive the set of Xs and Ys, and using that will derive $Q$. There will be an implementation that calculates the $Q$ columns when needed, that can be used when calculating all $Q$ isn't doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e08b30-7578-4d4d-92a3-5ceafa96e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualSVMProblem:\n",
    "\n",
    "    def __init__(self, Xs, Ys, C, epsilon = 1e-5, sparse = False):\n",
    "        self.X = Xs\n",
    "        self.Y = Ys\n",
    "        self.C = C\n",
    "        self.a = np.zeros_like(Ys, dtype = float)\n",
    "        self.e = np.ones_like(self.a)\n",
    "        self.epsilon = epsilon\n",
    "        self.sparse = sparse\n",
    "\n",
    "        if sparse:\n",
    "            self.calculated = [False] * self.Y.shape[0]\n",
    "            self.Q = {}\n",
    "        else:\n",
    "            mat = np.stack([Ys.T] * Xs.shape[1], axis = 1)\n",
    "            \n",
    "            Z = Xs * mat\n",
    "            self.Q = Z @ Z.T\n",
    "        \n",
    "        self.d = - self.e\n",
    "\n",
    "    def getA(self):\n",
    "        return self.a\n",
    "        \n",
    "    def getDerivative(self):\n",
    "        return self.d    \n",
    "\n",
    "    def getMostViolatingPair(self):\n",
    "        \n",
    "        directions = self.d / self.Y\n",
    "        min_idx = -1\n",
    "        min_value = np.inf\n",
    "        max_idx = -1\n",
    "        max_value = -np.inf\n",
    "        \n",
    "        R = directions.copy()\n",
    "        R[np.logical_and(self.a < self.epsilon, self.Y == -1)] = np.inf\n",
    "        R[np.logical_and(self.a > (self.C - self.epsilon), self.Y == 1)] = np.inf\n",
    "\n",
    "        S = directions.copy()\n",
    "        S[np.logical_and(self.a < self.epsilon, self.Y == 1)] = -np.inf\n",
    "        S[np.logical_and(self.a > (self.C - self.epsilon), self.Y == -1)] = -np.inf\n",
    "\n",
    "        min_idx = np.argmin(R)\n",
    "        max_idx = np.argmax(S)\n",
    "\n",
    "        if R[min_idx] == np.inf or S[max_idx] == -np.inf:\n",
    "            return None\n",
    "                \n",
    "        return (min_idx, max_idx)\n",
    "\n",
    "    def updateA(self, idx1, a1, idx2, a2):\n",
    "\n",
    "        if self.sparse:\n",
    "            self.d = self.getQColumn(idx1) * (a1 - self.a[idx1]) + self.getQColumn(idx2) * (a2 - self.a[idx2]) + self.d\n",
    "        else:\n",
    "            self.d = self.Q[idx1] * (a1 - self.a[idx1]) + self.Q[idx2] * (a2 - self.a[idx2]) + self.d\n",
    "        \n",
    "        self.a[idx1] = a1\n",
    "        self.a[idx2] = a2\n",
    "\n",
    "    def getParameters(self):\n",
    "        mat = np.stack([self.a.T * self.Y.T] * self.X.shape[1], axis = 1)\n",
    "        w = np.sum(mat * self.X, axis = 0)\n",
    "        b = 0\n",
    "        for i in range(len(self.a)):\n",
    "            if(self.a[i] < self.epsilon or self.a[i] > self.C - self.epsilon):\n",
    "                continue\n",
    "            b = 1 / self.Y[i] - w @ self.X[i]\n",
    "            break        \n",
    "        return (w, b)\n",
    "\n",
    "    def getQColumn(self, i):\n",
    "        if self.sparse:\n",
    "            if not self.calculated[i]:\n",
    "                self.Q[i] = self.Y[i] * (self.Y * (self.X @ self.X[i]))\n",
    "                self.calculated[i] = True\n",
    "        return self.Q[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b1bc9-527c-4a5d-8d18-36e71c4e337a",
   "metadata": {},
   "source": [
    "#### Testing the Implementation\n",
    "\n",
    "The following cell test some functionality used to solve SMO, and see if they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd03a6-1ec2-4f40-9708-ff03ab0c7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the code with some artificial data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([1, -1])\n",
    "C = 5\n",
    "\n",
    "# The safe way to calculate matrix Q\n",
    "Q = np.zeros((2, 2))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        Q[i][j] = Y[i] * Y[j] * X[i] @ X[j]\n",
    "\n",
    "problem = DualSVMProblem(X, Y, C)\n",
    "\n",
    "ok = True\n",
    "    \n",
    "if ((np.array([-1, -1]) != problem.getDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative function if wrong\")\n",
    "\n",
    "update = np.array([1, 2])\n",
    "problem.updateA(0, update[0], 1, update[1])\n",
    "\n",
    "if (norm(Q[0] - problem.getQColumn(0)) > 1e-3 or norm(Q[1] - problem.getQColumn(1)) > 1e-3):\n",
    "    ok = False\n",
    "    print(\"The matix calculation is wrong\")\n",
    "    \n",
    "if ((Q @ update - np.ones(2) != problem.getDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The updates of variables doesn't update the derivative in the right way\")\n",
    "\n",
    "elements = problem.getDerivative() / Y\n",
    "\n",
    "if not (problem.getMostViolatingPair()[0] == (0 if elements[0] < elements[1] else 1) and problem.getMostViolatingPair()[1] == (0 if elements[0] > elements[1] else 1)):\n",
    "    ok = False\n",
    "    print(\"The calculation of the most violating pair is wrong\")\n",
    "    # This test is a bit odd and not exaustive, but can help\n",
    "\n",
    "if ok:\n",
    "    print(\"Everything is working as expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7369272-b06d-47ea-9f7c-490ee7b7b4c6",
   "metadata": {},
   "source": [
    "## Implement SMO outer Loop\n",
    "\n",
    "After implementing core elements for the SMO algorithm, it's time to get them together and build the actual result.\n",
    "In this implementation, I realized a function that execute a single iteration of the algorithm, leaving the check of convergence outside. In this way, we can use different stopping conditions.\n",
    "\n",
    "`SMO_step` uses the SMO functionality to do a single optimization step, `trainingSMO` and `trainingSMOtimed` use a single step until the termination condition is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9a1bb-e679-46c6-8407-2b7170ade1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingSMO(problem: DualSVMProblem, step: Callable[[DualSVMProblem], bool], epsilon = 1e-5, verbose = False):\n",
    "    loss = []\n",
    "    validation = []\n",
    "    \n",
    "    while(True): \n",
    "        (min_idx, max_idx) = problem.getMostViolatingPair()\n",
    "        elements = problem.getDerivative() / problem.Y\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Stopping condition: {elements[max_idx] - elements[min_idx]}\")\n",
    "        if(elements[min_idx] + epsilon > elements[max_idx]):\n",
    "            break\n",
    "        \n",
    "        result = step(problem)            \n",
    "        (w, b) = problem.getParameters()\n",
    "        svm = SVM(w, b)\n",
    "        \n",
    "        if not result:\n",
    "            # If the step method give False as a result indicates an error or a stop condition (i.e. having a derivative under the tollerance)\n",
    "            break\n",
    "\n",
    "    return (loss, validation)\n",
    "    \n",
    "def trainingSMOtimed(Xs, Ys, C, step: Callable[[DualSVMProblem], bool], test_time: float, sparse = False):\n",
    "    total_time = 0\n",
    "    start_time = time.time()\n",
    "    problem = DualSVMProblem(Xs, Ys, C, sparse = sparse)\n",
    "    while(True): \n",
    "        (min_idx, max_idx) = problem.getMostViolatingPair()\n",
    "        result = step(problem)\n",
    "        if not result:\n",
    "            return -1\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        if (total_time > test_time):\n",
    "            return (total_time, problem)\n",
    "\n",
    "    return (loss, validation)\n",
    "\n",
    "def SMO_step(problem: DualSVMProblem, epsilon: float = 1e-5):\n",
    "    test = problem.getMostViolatingPair()\n",
    "    if(test is None):\n",
    "        raise Exception(\"The problem did not give a valid violating pair\")\n",
    "    (min_idx, max_idx) = (test[0], test[1])\n",
    "\n",
    "    direction = np.zeros_like(problem.a)\n",
    "    direction[min_idx] = 1 * problem.Y[min_idx]\n",
    "    direction[max_idx] = - 1 * problem.Y[max_idx] #In theory I should divide, but y is in {-1, 1} making the multiplication equivalent, but more efficient\n",
    "\n",
    "    derivative = problem.getDerivative()\n",
    "\n",
    "    b = problem.C - problem.a[min_idx] if direction[min_idx] > 0 else problem.a[min_idx]\n",
    "    b = min(b, problem.C - problem.a[max_idx] if direction[max_idx] > 0 else problem.a[max_idx])\n",
    "    \n",
    "    if b < epsilon:\n",
    "        return False\n",
    "\n",
    "    tmp = problem.getQColumn(min_idx) * problem.Y[min_idx] - problem.getQColumn(max_idx) * problem.Y[max_idx]   \n",
    "    value = direction.T @ tmp\n",
    "\n",
    "    if value > epsilon:\n",
    "        b = min(b, -(derivative.T @ direction) / value)\n",
    "\n",
    "    problem.updateA(min_idx, problem.a[min_idx] + b * direction[min_idx], max_idx, problem.a[max_idx] + b * direction[max_idx])\n",
    "    return True    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de661d59-f549-470d-bfa1-5b59e23c35f0",
   "metadata": {},
   "source": [
    "## Converting the datasets\n",
    "\n",
    "Give the way this notebook implements the operation, the classes are needed to be in $\\{-1, 1\\}$. So, in the following cell, the classes arrays are translated to meet this requirement.\n",
    "\n",
    "In addition to that, there is a resume of the principal statistical index of all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f51036a-2699-43c4-8c23-cfddcb56838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "      <td>683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.848653</td>\n",
       "      <td>-0.235074</td>\n",
       "      <td>-0.522043</td>\n",
       "      <td>-0.507727</td>\n",
       "      <td>-0.593298</td>\n",
       "      <td>-0.503498</td>\n",
       "      <td>-0.434521</td>\n",
       "      <td>-0.456646</td>\n",
       "      <td>-0.584513</td>\n",
       "      <td>-0.865951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.092696</td>\n",
       "      <td>0.626836</td>\n",
       "      <td>0.681143</td>\n",
       "      <td>0.664129</td>\n",
       "      <td>0.636569</td>\n",
       "      <td>0.494019</td>\n",
       "      <td>0.809746</td>\n",
       "      <td>0.544377</td>\n",
       "      <td>0.678370</td>\n",
       "      <td>0.385039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.878390</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.834453</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.824460</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  683.000000  683.000000  683.000000  683.000000  683.000000  683.000000   \n",
       "mean    -0.848653   -0.235074   -0.522043   -0.507727   -0.593298   -0.503498   \n",
       "std      0.092696    0.626836    0.681143    0.664129    0.636569    0.494019   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%     -0.878390   -0.777778   -1.000000   -1.000000   -1.000000   -0.777778   \n",
       "50%     -0.834453   -0.333333   -1.000000   -1.000000   -1.000000   -0.777778   \n",
       "75%     -0.824460    0.111111   -0.111111   -0.111111   -0.333333   -0.333333   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "                6           7           8           9  \n",
       "count  683.000000  683.000000  683.000000  683.000000  \n",
       "mean    -0.434521   -0.456646   -0.584513   -0.865951  \n",
       "std      0.809746    0.544377    0.678370    0.385039  \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000  \n",
       "25%     -1.000000   -0.777778   -1.000000   -1.000000  \n",
       "50%     -1.000000   -0.555556   -1.000000   -1.000000  \n",
       "75%      0.111111   -0.111111   -0.333333   -1.000000  \n",
       "max      1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Xs, Ys) = load_svmlight_file(\"datasets/breast_cancer_scale\")\n",
    "Ys[Ys == 2] = 1\n",
    "Ys[Ys == 4] = -1 \n",
    "\n",
    "Xs = Xs.toarray()\n",
    "\n",
    "dump_svmlight_file(Xs, Ys, \"datasets/breast_cancer_ones\")\n",
    "\n",
    "# There is a lot to say (Xs originally is a sparce matrix, not scaled values broke the model, Ys classes are 2 and 4)\n",
    "dataframe = pd.DataFrame(Xs)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a518ef-5fca-428d-9340-569198cf8b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.550458</td>\n",
       "      <td>0.432380</td>\n",
       "      <td>0.213693</td>\n",
       "      <td>0.192862</td>\n",
       "      <td>0.283487</td>\n",
       "      <td>0.330216</td>\n",
       "      <td>0.835221</td>\n",
       "      <td>0.879208</td>\n",
       "      <td>0.561135</td>\n",
       "      <td>0.276076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044175</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.140062</td>\n",
       "      <td>0.310871</td>\n",
       "      <td>0.113458</td>\n",
       "      <td>0.152147</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>0.219089</td>\n",
       "      <td>0.105393</td>\n",
       "      <td>0.077830</td>\n",
       "      <td>0.150687</td>\n",
       "      <td>0.184608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205483</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.475238</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.077309</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.838583</td>\n",
       "      <td>0.468504</td>\n",
       "      <td>0.142758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.568784</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.156049</td>\n",
       "      <td>0.262274</td>\n",
       "      <td>0.280596</td>\n",
       "      <td>0.858268</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.238394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.652326</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.274875</td>\n",
       "      <td>0.312661</td>\n",
       "      <td>0.467613</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.933071</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.550458       0.432380       0.213693       0.192862   \n",
       "std         0.140062       0.310871       0.113458       0.152147   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.475238       0.161111       0.136364       0.077309   \n",
       "50%         0.568784       0.352778       0.196970       0.156049   \n",
       "75%         0.652326       0.722222       0.272727       0.274875   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.283487       0.330216       0.835221       0.879208   \n",
       "std         0.075317       0.219089       0.105393       0.077830   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.232558       0.155403       0.779528       0.838583   \n",
       "50%         0.262274       0.280596       0.858268       0.889764   \n",
       "75%         0.312661       0.467613       0.909449       0.933071   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  8              9   ...             44             45  \\\n",
       "count  581012.000000  581012.000000  ...  581012.000000  581012.000000   \n",
       "mean        0.561135       0.276076  ...       0.044175       0.090392   \n",
       "std         0.150687       0.184608  ...       0.205483       0.286743   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.468504       0.142758  ...       0.000000       0.000000   \n",
       "50%         0.562992       0.238394  ...       0.000000       0.000000   \n",
       "75%         0.661417       0.355500  ...       0.000000       0.000000   \n",
       "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "                  46             47             48             49  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.077716       0.002773       0.003255       0.000205   \n",
       "std         0.267725       0.052584       0.056957       0.014310   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  50             51             52             53  \n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000  \n",
       "mean        0.000513       0.026803       0.023762       0.015060  \n",
       "std         0.022641       0.161508       0.152307       0.121791  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000       0.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Xs, Ys) = load_svmlight_file(\"datasets/covtype_scale\")\n",
    "Ys[Ys == 2] = -1\n",
    "\n",
    "Xs = Xs.toarray()\n",
    "\n",
    "dump_svmlight_file(Xs, Ys, \"datasets/covtype_ones\")\n",
    "dataframe = pd.DataFrame(Xs)\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfe3fd-1d40-498b-86d3-981c76c60526",
   "metadata": {},
   "source": [
    "## Testing the Code\n",
    "\n",
    "In the following cells, I will use a very simple classification problem to see if the code works at all. [This dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#breast-cancer) is composed by only 683 elements, everyone of only 10 features. This dataset is chosen dues to it contained dimension. In fact, this implementation of the Dual SVM explicitly calculate the Q matrix, something that can become prohibitive very quickly in the increase of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527adb5-0227-4932-b206-5e3ca3be7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_Xs, test_Xs, train_Ys, test_Ys)  = train_test_split(Xs, Ys, test_size = 0.20)\n",
    "\n",
    "base_svm = LinearSVC(C = 10)\n",
    "base_svm.fit(train_Xs, train_Ys)\n",
    "\n",
    "train_preds = base_svm.predict(train_Xs)\n",
    "test_preds = base_svm.predict(test_Xs)\n",
    "\n",
    "\n",
    "print(\"Report on train set:\")\n",
    "print(classification_report(train_Ys, train_preds, zero_division = 0))\n",
    "\n",
    "print(\"Report on test set:\")\n",
    "print(classification_report(test_Ys, test_preds, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff7ffa-a036-4b5c-b3b8-84e12eb643c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_smo_problem = DualSVMProblem(train_Xs, train_Ys, C = 10)\n",
    "\n",
    "(_, _) = trainingSMO(base_smo_problem, SMO_step, epsilon = 1e-2)\n",
    "\n",
    "(w, b) = base_smo_problem.getParameters()\n",
    "smo_svm = SVM(w, b)\n",
    "smo_train_preds = smo_svm(train_Xs)\n",
    "smo_test_preds = smo_svm(test_Xs)\n",
    "\n",
    "print(\"Result of the model trained using SMO on training set:\")\n",
    "print(classification_report(train_Ys, smo_train_preds, zero_division=0))\n",
    "\n",
    "print(\"Result of the model trained using SMO on test set:\")\n",
    "print(classification_report(test_Ys, smo_test_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbb6aa-7669-498a-9822-a016c40b09ed",
   "metadata": {},
   "source": [
    "Those simple test are not so determinant, but can give some insight on how much the obtained model differ from the `LinearSVC` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d003af-5138-4bef-9e55-a9d98904a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Difference in norm of w: {norm(w - base_svm.coef_)}\")\n",
    "print(f'\"Correlation\" between the direction of ws : {(w.T @ base_svm.coef_[0]) / (norm(w) * norm(base_svm.coef_[0]))}')\n",
    "print(f\"Difference in the value b: {abs(b - base_svm.intercept_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87110bed-84a9-4d1a-bb9a-646ca89a9935",
   "metadata": {},
   "source": [
    "## Verifying if the sparse implementation gives some benefits\n",
    "\n",
    "Here there is a test that show if valuating $Q$ in a lazy way makes some differences in time to train. Obviously, the main difference is the memory footprint.\n",
    "\n",
    "To sum up, when possible calculating upfront $Q$ it's easier and slightly more efficient (at least equivalent), but with higher dimension  it's not doable anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8aacc-4f34-443a-b231-89ad8520267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A, B) = load_dataset(\"datasets/covtype_ones\", dimension = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff753d-f0ae-4c31-b5e6-a9eb8c024357",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "unsparse_smo_problem = DualSVMProblem(A[0], A[1], C = 1)\n",
    "(_, _) = trainingSMO(unsparse_smo_problem, SMO_step, epsilon = 1e-2)\n",
    "time_taken = time.time() - start_time\n",
    "print(f\"Time Taken by the base method: {time_taken}\")\n",
    "print(\"Report on Performance\")\n",
    "(w, b) = unsparse_smo_problem.getParameters()\n",
    "svm = SVM(w, b)\n",
    "preds = svm(A[0])\n",
    "print(classification_report(A[1], preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c39bb1-96d8-4f6d-9b1e-cb7344434af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sparse_smo_problem = DualSVMProblem(A[0], A[1], C = 1, sparse = True)\n",
    "(_, _) = trainingSMO(sparse_smo_problem, SMO_step, epsilon = 1e-2)\n",
    "time_taken = time.time() - start_time\n",
    "print(f\"Time Taken by the sparce method: {time_taken}\")\n",
    "print(\"Report on Performance\")\n",
    "(w, b) = sparse_smo_problem.getParameters()\n",
    "svm = SVM(w, b)\n",
    "preds = svm(A[0])\n",
    "print(classification_report(A[1], preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b86a08-a5b2-44b8-8e68-b8ec942034e1",
   "metadata": {},
   "source": [
    "## DCD Linear\n",
    "\n",
    "After implementing the SMO algorithm, now it's time to implement the DCD version. To maintain the uniformity in the problem definition, I will implement the dual formulation of the L1-SVM problem. \n",
    "\n",
    "In this case the main functionality to implement are:\n",
    "* Optimizing over a single variable\n",
    "* Recover the projected derivative\n",
    "* Updating `w` at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba695671-5aa8-4deb-892a-081b7671c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDDualProblem:\n",
    "\n",
    "    def __init__(self, Xs, Ys, C, epsilon = 1e-5):\n",
    "        self.X = Xs\n",
    "        self.Y = Ys\n",
    "        self.C = C\n",
    "        self.a = np.zeros_like(Ys, dtype = float)\n",
    "        self.e = np.ones_like(Ys)\n",
    "        self.diagQ = np.pow(np.sum(np.pow(Xs, 2), axis = 1, dtype = float), -1) #Inverse Diagonal of Q\n",
    "\n",
    "        self.w = np.zeros(Xs.shape[1], dtype = float)\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def optimizeOver(self, index):\n",
    "        G = self.Y[index] * self.w.T @ self.X[index] - 1\n",
    "\n",
    "        if self.a[index] < self.epsilon:\n",
    "            PG = min(G, 0)\n",
    "        elif self.a[index] > self.C - self.epsilon:\n",
    "            PG = max(G, 0)\n",
    "        else:\n",
    "            PG = G\n",
    "\n",
    "        if abs(PG) < self.epsilon:\n",
    "            return\n",
    "        \n",
    "        #Notice that during initialization of the class diagQ is already inverted. In this way the code should be more efficient\n",
    "        new_a = min(max(self.a[index] - G * self.diagQ[index], 0), self.C) \n",
    "        self.w = self.w + (new_a - self.a[index]) * self.Y[index] * self.X[index]\n",
    "        self.a[index] = new_a\n",
    "        \n",
    "    def getProjectedDerivative(self):        \n",
    "        derivative = self.Y * (self.X @ self.w) - self.e\n",
    "        derivative[np.logical_and(self.a < self.epsilon, derivative > 0)] = 0\n",
    "        derivative[np.logical_and(self.a > self.C - self.epsilon, derivative < 0)] = 0\n",
    "        return derivative\n",
    "\n",
    "    def getParameters(self):\n",
    "        return self.w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3a3e4-b2e8-4a48-930a-15acd700d674",
   "metadata": {},
   "source": [
    "#### Performing some tests\n",
    "\n",
    "As before, there are some tests to assure that everything works as intend. Those test are (obviously) not exhaustive, but are an initial step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2accf-e0ef-47c3-a5be-3d056cabf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the code with some artificial data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "Y = np.array([1, -1])\n",
    "C = 5\n",
    "\n",
    "diagQ = np.array([1 / (X[0].T @ X[0]), 1 / (X[1].T @ X[1])])\n",
    "\n",
    "problem = DCDDualProblem(X, Y, C)\n",
    "\n",
    "ok = True\n",
    "if (not (diagQ == problem.diagQ).all()):\n",
    "    ok = False\n",
    "    print(\"The diagonal of the matix is wrong\")\n",
    "\n",
    "if (not (-np.ones(2) == problem.getProjectedDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (in the initial state)\")\n",
    "\n",
    "problem.optimizeOver(0)\n",
    "\n",
    "if (not (np.array([1 / 14, 0]) == problem.a).all()):\n",
    "    ok = False\n",
    "    print(\"The optimization function doesn't work.\")\n",
    "\n",
    "derivative = np.array([Y[i] * problem.w.T @ X[i] - 1 for i in range(X.shape[0])])\n",
    "if (not (derivative == problem.getProjectedDerivative()).all()):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (after update)\")\n",
    "\n",
    "if (norm(np.array([1, 2, 3]) / 14 - problem.w) > 10e-5):\n",
    "    ok = False\n",
    "    print(f\"The w is wrong {problem.w}\")\n",
    "\n",
    "### Tests on second update\n",
    "problem.optimizeOver(1)\n",
    "if (norm(np.array([1, 46 / 77]) / 14 - problem.a) > 1e-5):\n",
    "    ok = False\n",
    "    print(\"The optimization function doesn't work (second update).\")\n",
    "\n",
    "derivative = np.array([Y[i] * problem.w.T @ X[i] - 1 for i in range(X.shape[0])])\n",
    "if (norm(derivative - problem.getProjectedDerivative()) > 1e-5):\n",
    "    ok = False\n",
    "    print(\"The derivative calcultaion is wrong (second update)\")\n",
    "\n",
    "if ok:\n",
    "    print(\"Everything is working as expected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741f2a8-320c-4f30-b25f-9a76f1024fa9",
   "metadata": {},
   "source": [
    "### Implement DCD outer Loop\n",
    "\n",
    "As before, the single step and outer loop are separated to allow different stopping condition but with the same optimization step. Note that, due to a different interface and different definition of the stopping condition, the SMOs outer loops can't be used on DCD formulation, and otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5714d161-2009-4013-8bc9-37baaee82df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingDCD(problem: DCDDualProblem, epsilon = 10e-5, verbose: bool = False):\n",
    "    loss = []\n",
    "    validation = []\n",
    "    while(norm(problem.getProjectedDerivative()) > epsilon):\n",
    "        if verbose:\n",
    "            print(f\"Norm:{norm(problem.getProjectedDerivative())}\")\n",
    "        for i in np.random.permutation(len(problem.a)):\n",
    "            problem.optimizeOver(i)\n",
    "            w = problem.getParameters()\n",
    "            svm = SVM(w, 0)\n",
    "    return (loss, validation)\n",
    "\n",
    "def trainingDCDadvanced(problem: DCDDualProblem, epsilon = 1e-5, verbose: bool = False):\n",
    "    loss = []\n",
    "    validation = []\n",
    "    M = np.inf\n",
    "    m = -np.inf\n",
    "    while(True):\n",
    "        for i in np.random.permutation(len(problem.a)):\n",
    "            \n",
    "            problem.optimizeOver(i)\n",
    "            w = problem.getParameters()\n",
    "            svm = SVM(w, 0)\n",
    "                \n",
    "        M = np.max(problem.getProjectedDerivative())\n",
    "        m = np.min(problem.getProjectedDerivative())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Condition :{M - m}\")\n",
    "            \n",
    "        if M - m < epsilon:\n",
    "            return (loss, validation)\n",
    "\n",
    "        if M <= 0:\n",
    "            M = np.inf\n",
    "        if m >= 0:\n",
    "            m = -np.inf\n",
    "            \n",
    "def trainingDCDtimed(Xs, Ys, C, test_time = float):\n",
    "    start_time = time.time()\n",
    "    total_time = 0\n",
    "    problem = DCDDualProblem(Xs, Ys, C)\n",
    "    while(True):\n",
    "        for i in np.random.permutation(len(problem.a)):            \n",
    "            problem.optimizeOver(i)\n",
    "            total_time = time.time() - start_time\n",
    "            if (total_time > test_time):\n",
    "                return (total_time, problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5764cfc-b80c-41fa-837c-888df4243de1",
   "metadata": {},
   "source": [
    "Notice that for using DCD we need to add a fictitious feature that is constant to 1, so to model the bias as a model parameter. Only now time this operation is done explicitly, so far it will be embedded in the utilities functions defined on top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c497f-bb3e-478b-a28f-4e2a81842aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_bias_train = np.atleast_2d(np.ones(train_Xs.shape[0])).T\n",
    "explicit_bias_test = np.atleast_2d(np.ones(test_Xs.shape[0])).T\n",
    "\n",
    "train_Xs_unbiased = np.hstack([train_Xs, explicit_bias_train])\n",
    "test_Xs_unbiased = np.hstack([test_Xs, explicit_bias_test])\n",
    "\n",
    "print(train_Xs_unbiased.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa503204-d65f-4476-982e-7870a2410802",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dcd_problem = DCDDualProblem(train_Xs_unbiased, train_Ys, C = 10)\n",
    "\n",
    "(_, _) = trainingDCD(base_dcd_problem, epsilon = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290bb0b-3b8d-4cea-8cb9-33b5acbf09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = base_dcd_problem.getParameters()\n",
    "\n",
    "dcd_svm = SVM(w, 0)\n",
    "dcd_train_preds = dcd_svm(train_Xs_unbiased)\n",
    "dcd_test_preds = dcd_svm(test_Xs_unbiased)\n",
    "\n",
    "print(\"Result of the model trained using DCD on training set:\")\n",
    "print(classification_report(train_Ys, dcd_train_preds, zero_division=0))\n",
    "\n",
    "print(\"Result of the model trained using DCD on test set:\")\n",
    "print(classification_report(test_Ys, dcd_test_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682e889-c5f0-4b0a-9b59-86ebe4c00514",
   "metadata": {},
   "source": [
    "## Verifying that the two model give good results\n",
    "\n",
    "Here there is a first comparative test on accuracy performance, to ensure that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e471b5-0853-4f09-803a-f9d5e3173758",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bias, unbias) = load_dataset(\"datasets/covtype_ones\", dimension = 1000)\n",
    "\n",
    "C = 1\n",
    "threshold = 1e-1\n",
    "\n",
    "smo_problem = DualSVMProblem(bias[0], bias[1], C = C)\n",
    "(_, _) = trainingSMO(smo_problem, SMO_step, epsilon = threshold)\n",
    "\n",
    "(w, b) = smo_problem.getParameters()\n",
    "svm = SVM(w, b)\n",
    "preds = svm(bias[0])\n",
    "print(\"Result of the model trained using SMO on training set:\")\n",
    "print(classification_report(bias[1], preds, zero_division=0))\n",
    "\n",
    "dcd_problem = DCDDualProblem(unbias[0], unbias[1], C = C)\n",
    "(_, _) = trainingDCD(dcd_problem, epsilon = threshold)\n",
    "w = dcd_problem.getParameters()        \n",
    "svm = SVM(w, 0)\n",
    "preds = svm(unbias[0])\n",
    "print(\"Result of the model trained using DCD on training set:\")\n",
    "print(classification_report(unbias[1], preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d1f2f-0256-4562-9f24-32cb24328313",
   "metadata": {},
   "source": [
    "# Testing Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99701d3-4e26-439d-bab2-e31e6fc9fddb",
   "metadata": {},
   "source": [
    "## Time taken changing dataset dimension, while fixing the tolerance on stopping condition.\n",
    "\n",
    "The main test consist of executing the two algorithm with different dataset sizes and tolerances on the stopping condition. Then, the training/test accuracy and time taken are measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33789739-6fe4-4272-a288-e64e5b5bd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_tests(dataset, dimensions, thresholds, C, test_size):\n",
    "    smo_results = {}\n",
    "    dcd_results = {}\n",
    "    \n",
    "    smo_results[\"dimension\"] = []\n",
    "    dcd_results[\"dimension\"] = []\n",
    "    \n",
    "    for d in dimensions:\n",
    "        (bias, unbias) = load_dataset(f\"datasets/{dataset}\", dimension = d)\n",
    "    \n",
    "        (W, X, Y, Z) = train_test_split(bias[0], bias[1], test_size = test_size)\n",
    "        bias = (W, Y)\n",
    "        bias_test = (X, Z)\n",
    "        smo_results[\"dimension\"].append(bias[0].shape[0])\n",
    "        \n",
    "        (W, X, Y, Z) = train_test_split(unbias[0], unbias[1], test_size = test_size)\n",
    "        unbias = (W, Y)\n",
    "        unbias_test = (X, Z)\n",
    "        dcd_results[\"dimension\"].append(unbias[0].shape[0])\n",
    "    \n",
    "        smo_results[\"thresholds\"] = thresholds\n",
    "        smo_results[\"times\"] = []\n",
    "        smo_results[\"primal_loss\"] = []\n",
    "        smo_results[\"dual_loss\"] = []\n",
    "        smo_results[\"train_accuracy\"] = []\n",
    "        smo_results[\"test_accuracy\"] = []\n",
    "        \n",
    "        dcd_results[\"thresholds\"] = thresholds\n",
    "        dcd_results[\"times\"] = []\n",
    "        dcd_results[\"primal_loss\"] = []\n",
    "        dcd_results[\"dual_loss\"] = []\n",
    "        dcd_results[\"train_accuracy\"] = []\n",
    "        dcd_results[\"test_accuracy\"] = []\n",
    "        \n",
    "        mat = np.stack([bias[1].T] * bias[0].shape[1], axis = 1)    \n",
    "        Z = bias[0] * mat\n",
    "        biasQ = Z @ Z.T\n",
    "    \n",
    "        mat = np.stack([unbias[1].T] * unbias[0].shape[1], axis = 1)    \n",
    "        Z = unbias[0] * mat\n",
    "        unbiasQ = Z @ Z.T\n",
    "    \n",
    "        for t in thresholds:\n",
    "            print(f\"Dataset: {dataset}\", f\"\\tDimension: {d}\", f\"\\tThreshold: {t}\")\n",
    "            start_time = time.time()\n",
    "            smo_problem = DualSVMProblem(bias[0], bias[1], C = C)        \n",
    "            (_, _) = trainingSMO(smo_problem, SMO_step, epsilon = t)\n",
    "            smo_time = time.time() - start_time\n",
    "            \n",
    "            (w, b) = smo_problem.getParameters()\n",
    "            svm = SVM(w, b)\n",
    "            \n",
    "            smo_results[\"primal_loss\"].append(loss_svm(w, b, C, bias[0], bias[1]))\n",
    "            smo_results[\"dual_loss\"].append(dual_loss_svm(smo_problem.a, biasQ))\n",
    "            smo_results[\"times\"].append(smo_time)\n",
    "            smo_results[\"train_accuracy\"].append(calculate_accuracy(bias[1], bias[0], svm))\n",
    "            smo_results[\"test_accuracy\"].append(calculate_accuracy(bias_test[1], bias_test[0], svm))\n",
    "\n",
    "            start_time = time.time()\n",
    "            dcd_problem = DCDDualProblem(unbias[0], unbias[1], C = C)\n",
    "            (_, _) = trainingDCDadvanced(dcd_problem, epsilon = t)\n",
    "            dcd_time = time.time() - start_time\n",
    "            \n",
    "            w = dcd_problem.getParameters()        \n",
    "            svm = SVM(w, 0)\n",
    "            dcd_results[\"primal_loss\"].append(loss_svm(w, 0, C, unbias[0], unbias[1]))\n",
    "            dcd_results[\"dual_loss\"].append(dual_loss_svm(dcd_problem.a, unbiasQ))\n",
    "            dcd_results[\"times\"].append(dcd_time)\n",
    "            dcd_results[\"train_accuracy\"].append(calculate_accuracy(unbias[1], unbias[0], svm))\n",
    "            dcd_results[\"test_accuracy\"].append(calculate_accuracy(unbias_test[1], unbias_test[0], svm))\n",
    "    \n",
    "        smo_file = open(f\"Results/smo_{d}_{C}.json\", \"w\")\n",
    "        smo_file.write(json.dumps(smo_results, indent = 2))\n",
    "        smo_file.close()\n",
    "        \n",
    "        dcd_file = open(f\"Results/dcd_{d}_{C}.json\", \"w\")\n",
    "        dcd_file.write(json.dumps(dcd_results, indent = 2))\n",
    "        dcd_file.close()\n",
    "    print(\"Test Finished\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1915913-37d0-46e3-854a-76abc60b6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"covtype_ones\"\n",
    "thresholds = [1/(i + 1) ** 2 for i in range(10)]\n",
    "test_size = 0.2\n",
    "dimensions = [math.floor(i / (1 - test_size)) for i in range(500, 10001, 500)]\n",
    "\n",
    "if not os.path.exists(\"Results\"):\n",
    "    os.mkdir(\"Results\")\n",
    "main_tests(dataset, dimensions, thresholds, 1, 0.2)\n",
    "main_tests(dataset, dimensions, thresholds, 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d085b-2615-4126-9136-095bc7061da8",
   "metadata": {},
   "source": [
    "### Results Over Dimension\n",
    "\n",
    "In these plots, there will be shown how training time changes while increasing the dataset dimension, but the thresholds remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a0af6-4fff-4396-936e-f8bbaff6058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Indicates the position where to read the time values that are associated to a specific threshold.\n",
    "def plot_over_dimension(dimensions, C, index, save_fig = None):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    smo_dimensions = []\n",
    "    smo_times = []\n",
    "    \n",
    "    dcd_dimensions = []\n",
    "    dcd_times = []\n",
    "    \n",
    "    for d in dimensions:\n",
    "        smo_file = open(f\"Results/smo_{d}_{C}.json\", \"r\")\n",
    "        smo_results = json.loads(smo_file.read())\n",
    "        smo_times.append(smo_results[\"times\"][index])\n",
    "        smo_file.close()\n",
    "        \n",
    "        dcd_file = open(f\"Results/dcd_{d}_{C}.json\", \"r\")\n",
    "        dcd_results = json.loads(dcd_file.read())\n",
    "        dcd_times.append(dcd_results[\"times\"][index])\n",
    "        dcd_file.close()\n",
    "    \n",
    "    plt.plot(dimensions, smo_times, label = \"SMO\", marker='o', linestyle='dashed')\n",
    "    plt.plot(dimensions, dcd_times, label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    \n",
    "    plt.xlabel(\"Dataset Dimension [#]\")\n",
    "    plt.ylabel(\"Time Taken [s]\")\n",
    "    plt.title(f\"Time taken with threshold {round(smo_results[\"thresholds\"][index], 2)} (C = {C})\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "\n",
    "    plt.show()\n",
    "plot_over_dimension(dimensions, 1, 0, save_fig = \"dimension_1.png\")\n",
    "plot_over_dimension(dimensions, 1, 3, save_fig = \"dimension_2.png\")\n",
    "plot_over_dimension(dimensions, 1, 6, save_fig = \"dimension_3.png\")\n",
    "plot_over_dimension(dimensions, 1, 9, save_fig = \"dimension_4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff29f6-47fc-42c6-988f-e76e702fc450",
   "metadata": {},
   "source": [
    "### Relation between dimension and accuracy\n",
    "\n",
    "Here, there will be correlated accuracy with the dataset dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4f755-6914-47a6-a56e-2cedd77b6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_dimension_accuracy(dimensions, C, index, save_fig = None, train = False):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    smo_dimensions = []\n",
    "    smo_acc = []\n",
    "    \n",
    "    dcd_dimensions = []\n",
    "    dcd_acc = []\n",
    "    \n",
    "    for d in dimensions:\n",
    "        smo_file = open(f\"Results/smo_{d}_{C}.json\", \"r\")\n",
    "        smo_results = json.loads(smo_file.read())\n",
    "        if train:\n",
    "            smo_acc.append(smo_results[\"train_accuracy\"][index])\n",
    "        else:\n",
    "            smo_acc.append(smo_results[\"test_accuracy\"][index])\n",
    "            \n",
    "        smo_file.close()\n",
    "        \n",
    "        dcd_file = open(f\"Results/dcd_{d}_{C}.json\", \"r\")\n",
    "        dcd_results = json.loads(dcd_file.read())\n",
    "        if train:\n",
    "            dcd_acc.append(dcd_results[\"train_accuracy\"][index])\n",
    "        else:\n",
    "            dcd_acc.append(dcd_results[\"test_accuracy\"][index])\n",
    "        dcd_file.close()\n",
    "        \n",
    "    plt.plot(dimensions, smo_acc, label = \"SMO\", marker='o', linestyle='dashed')\n",
    "    plt.plot(dimensions, dcd_acc, label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    \n",
    "    plt.xlabel(\"Dataset Dimension [#]\")\n",
    "    if train:\n",
    "        plt.ylabel(\"Train accuracy\")\n",
    "        plt.title(f\"Train accuracy with threshold {round(smo_results[\"thresholds\"][index], 2)} (C = {C})\")\n",
    "    else:\n",
    "        plt.ylabel(\"Test accuracy\")\n",
    "        plt.title(f\"Test accuracy with threshold {round(smo_results[\"thresholds\"][index], 2)} (C = {C})\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "    plt.show()\n",
    "plot_over_dimension_accuracy(dimensions, 1, 0, save_fig = \"accuracy_dimension_1.png\")\n",
    "plot_over_dimension_accuracy(dimensions, 1, 9, save_fig = \"accuracy_dimension_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e34630-d039-4291-bfe5-25060c2b1155",
   "metadata": {},
   "source": [
    "## Correlation between Threshold and Time\n",
    "\n",
    "In these plots there will be the amount of time required to reach a certain tollerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8866e6b-a5c8-4dff-9f2c-50cdc283360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_threshold(dimension, C, save_fig = None):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    smo_file = open(f\"Results/smo_{dimension}_{C}.json\", \"r\")\n",
    "    smo_results = json.loads(smo_file.read())\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_{dimension}_{C}.json\", \"r\")\n",
    "    dcd_results = json.loads(dcd_file.read())\n",
    "    dcd_file.close()\n",
    "    \n",
    "    plt.plot(smo_results[\"times\"], smo_results[\"thresholds\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "    plt.plot(dcd_results[\"times\"], dcd_results[\"thresholds\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    \n",
    "    plt.ylabel(\"Threshold [#]\")\n",
    "    plt.xlabel(\"Time Taken [s]\")\n",
    "    plt.title(f\"Time taken with {dimension} elements (C = {C})\")\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "\n",
    "plot_over_threshold(6250, 1, save_fig = \"threshold_1.png\")\n",
    "plot_over_threshold(10000, 1, save_fig = \"threshold_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f804e4-2c7a-4b9b-8405-3f2140fbcd97",
   "metadata": {},
   "source": [
    "### Correlation between Accuracy and Tolerance\n",
    "\n",
    "Here instead, we will plot the correlation between Accuracy and Tolerance (Threshold) at a given dataset dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c0000-2c04-4c45-a4ed-1e0d1da943ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_accuracy(dimension, C, save_fig = None, train = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    smo_file = open(f\"Results/smo_{dimension}_{C}.json\", \"r\")\n",
    "    smo_results = json.loads(smo_file.read())\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_{dimension}_{C}.json\", \"r\")\n",
    "    dcd_results = json.loads(dcd_file.read())\n",
    "    dcd_file.close()\n",
    "\n",
    "    if train:\n",
    "        plt.plot(smo_results[\"train_accuracy\"], smo_results[\"thresholds\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "        plt.plot(dcd_results[\"train_accuracy\"], dcd_results[\"thresholds\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    else:\n",
    "        plt.plot(smo_results[\"test_accuracy\"], smo_results[\"thresholds\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "        plt.plot(dcd_results[\"test_accuracy\"], dcd_results[\"thresholds\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    \n",
    "    plt.ylabel(\"Threshold [#]\")\n",
    "    if train:\n",
    "        plt.xlabel(\"Train Accuracy\")\n",
    "        plt.title(f\"Train accuracy with {dimension} elements (C = {C})\")\n",
    "    else:\n",
    "        plt.xlabel(\"Test Accuracy\")\n",
    "        plt.title(f\"Test accuracy with {dimension} elements (C = {C})\")\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "            \n",
    "plot_over_accuracy(1250, 1, save_fig = \"accuracy_extra.png\", train = True)\n",
    "plot_over_accuracy(6250, 1, save_fig = \"accuracy_1.png\", train = True)\n",
    "plot_over_accuracy(10000, 1, save_fig = \"accuracy_2.png\", train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950b1a0-fdff-4b36-a69c-c07f17a35487",
   "metadata": {},
   "source": [
    "## Test Over different feature Number\n",
    "\n",
    "In this test cases, we want to verify how training time changes with the number of feature considered. So, dataset dimension and tolerance remains fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99016e21-f559-4f65-9e70-346f9d7bcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_test(dataset, dimension, threshold, original_num, features_num, C, test_size):\n",
    "    smo_results = {}\n",
    "    smo_results[\"thresholds\"] = threshold\n",
    "    smo_results[\"features\"] = features_num\n",
    "    smo_results[\"times\"] = []\n",
    "    smo_results[\"primal_loss\"] = []\n",
    "    smo_results[\"dual_loss\"] = []\n",
    "    smo_results[\"train_accuracy\"] = []\n",
    "    smo_results[\"test_accuracy\"] = []\n",
    "    \n",
    "    dcd_results = {}\n",
    "    dcd_results[\"thresholds\"] = threshold\n",
    "    dcd_results[\"features\"] = features_num\n",
    "    dcd_results[\"times\"] = []\n",
    "    dcd_results[\"primal_loss\"] = []\n",
    "    dcd_results[\"dual_loss\"] = []\n",
    "    dcd_results[\"train_accuracy\"] = []\n",
    "    dcd_results[\"test_accuracy\"] = []\n",
    "    \n",
    "    \n",
    "    for f in features_num:\n",
    "        features = feature_extractor(f, original_num)    \n",
    "        (bias, unbias) = load_dataset(f\"datasets/{dataset}\", dimension = dimension, features = features)\n",
    "    \n",
    "        (W, X, Y, Z) = train_test_split(bias[0], bias[1], test_size = test_size)\n",
    "        bias = (W, Y)\n",
    "        bias_test = (X, Z)\n",
    "        smo_results[\"dimension\"] = bias[0].shape[0]\n",
    "        \n",
    "        (W, X, Y, Z) = train_test_split(unbias[0], unbias[1], test_size = test_size)\n",
    "        unbias = (W, Y)\n",
    "        unbias_test = (X, Z)\n",
    "        dcd_results[\"dimension\"] = unbias[0].shape[0]\n",
    "        \n",
    "        mat = np.stack([bias[1].T] * bias[0].shape[1], axis = 1)    \n",
    "        Z = bias[0] * mat\n",
    "        biasQ = Z @ Z.T\n",
    "    \n",
    "        mat = np.stack([unbias[1].T] * unbias[0].shape[1], axis = 1)    \n",
    "        Z = unbias[0] * mat\n",
    "        unbiasQ = Z @ Z.T\n",
    "        \n",
    "        print(f\"Dimension: {dimension}\",f\"\\tThreshold: {threshold}\", f\"\\tFeatures {f}\")\n",
    "            \n",
    "        start_time = time.time()\n",
    "        smo_problem = DualSVMProblem(bias[0], bias[1], C)        \n",
    "        (_, _) = trainingSMO(smo_problem, SMO_step, epsilon = threshold)\n",
    "        smo_time = time.time() - start_time\n",
    "        \n",
    "        (w, b) = smo_problem.getParameters()\n",
    "        svm = SVM(w, b)\n",
    "    \n",
    "        smo_results[\"primal_loss\"].append(loss_svm(w, b, C, bias[0], bias[1]))\n",
    "        smo_results[\"dual_loss\"].append(dual_loss_svm(smo_problem.a, biasQ))\n",
    "        smo_results[\"times\"].append(smo_time)\n",
    "        smo_results[\"train_accuracy\"].append(calculate_accuracy(bias[1], bias[0], svm))\n",
    "        smo_results[\"test_accuracy\"].append(calculate_accuracy(bias_test[1], bias_test[0], svm))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        dcd_problem = DCDDualProblem(unbias[0], unbias[1], C)\n",
    "        (_, _) = trainingDCDadvanced(dcd_problem, epsilon = threshold)\n",
    "        dcd_time = time.time() - start_time\n",
    "        \n",
    "        w = dcd_problem.getParameters()        \n",
    "        svm = SVM(w, 0)\n",
    "        \n",
    "        dcd_results[\"primal_loss\"].append(loss_svm(w, 0, C, unbias[0], unbias[1]))\n",
    "        dcd_results[\"dual_loss\"].append(dual_loss_svm(dcd_problem.a, unbiasQ))\n",
    "        dcd_results[\"times\"].append(dcd_time)\n",
    "        dcd_results[\"train_accuracy\"].append(calculate_accuracy(unbias[1], unbias[0], svm))\n",
    "        dcd_results[\"test_accuracy\"].append(calculate_accuracy(unbias_test[1], unbias_test[0], svm))\n",
    "        \n",
    "    smo_file = open(f\"Results/smo_features_{dimension}_{C}.json\", \"w\")\n",
    "    smo_file.write(json.dumps(smo_results, indent = 2))\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_features_{dimension}_{C}.json\", \"w\")\n",
    "    dcd_file.write(json.dumps(dcd_results, indent = 2))\n",
    "    dcd_file.close()\n",
    "    print(\"Test Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204083b4-7599-4e28-83ba-56ef90981953",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-2\n",
    "dataset = \"covtype_ones\"\n",
    "original_num = 54\n",
    "features_num = [i for i in range(19, 55, 5)]\n",
    "test_size = 0.20\n",
    "\n",
    "feature_test(dataset, 10000, threshold, original_num, features_num, 1, 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c4fb4-27af-40e5-9299-43411fd44478",
   "metadata": {},
   "source": [
    "### Plot Changing Feature Number\n",
    "\n",
    "These plots simply correlate the training time with the feature number changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bde42-95a1-4001-a01c-97baa2729c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_feature(dimension, C, save_fig = None):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    smo_file = open(f\"Results/smo_features_{dimension}_{C}.json\", \"r\")\n",
    "    smo_results = json.loads(smo_file.read())\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_features_{dimension}_{C}.json\", \"r\")\n",
    "    dcd_results = json.loads(dcd_file.read())\n",
    "    dcd_file.close()\n",
    "    \n",
    "    plt.plot(smo_results[\"features\"], smo_results[\"times\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "    plt.plot(dcd_results[\"features\"], dcd_results[\"times\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    \n",
    "    plt.xlabel(\"feature Number [#]\")\n",
    "    plt.ylabel(\"Time Taken [s]\")\n",
    "    plt.title(f\"Threashold {smo_results[\"thresholds\"]}, {smo_results[\"dimension\"]} elements,  C = {C}\")\n",
    "    \n",
    "    plt.legend()\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_over_feature(10000, 1, save_fig = \"features_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df6836-a1e6-4e0f-ab6e-0572a9b73e9e",
   "metadata": {},
   "source": [
    "## Test allocating a determinate amount of time\n",
    "\n",
    "Lastly, we will test how much accuracy an algorithm can reach in a give time. In this case, there is a test were only DCD are executed, because on big dataset is useless testing SMO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123cd76-7564-4b58-86f7-c33c89acbc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_test(dataset, dimension, times, C, test_size, sparse = False):\n",
    "    smo_results = {}\n",
    "    dcd_results = {}\n",
    "    \n",
    "    smo_results[\"dimension\"] = []\n",
    "    dcd_results[\"dimension\"] = []\n",
    "    (bias, unbias) = load_dataset(f\"datasets/{dataset}\", dimension = dimension)\n",
    "\n",
    "    (W, X, Y, Z) = train_test_split(bias[0], bias[1], test_size = test_size)\n",
    "    bias = (W, Y)\n",
    "    bias_test = (X, Z)\n",
    "    smo_results[\"dimension\"] = bias[0].shape[0]\n",
    "    \n",
    "    (W, X, Y, Z) = train_test_split(unbias[0], unbias[1], test_size = test_size)\n",
    "    unbias = (W, Y)\n",
    "    unbias_test = (X, Z)\n",
    "    dcd_results[\"dimension\"] = unbias[0].shape[0]\n",
    "\n",
    "    smo_results[\"times\"] = []\n",
    "    smo_results[\"primal_loss\"] = []\n",
    "    smo_results[\"dual_loss\"] = []\n",
    "    smo_results[\"train_accuracy\"] = []\n",
    "    smo_results[\"test_accuracy\"] = []\n",
    "    \n",
    "    dcd_results[\"times\"] = []\n",
    "    dcd_results[\"primal_loss\"] = []\n",
    "    dcd_results[\"dual_loss\"] = []\n",
    "    dcd_results[\"train_accuracy\"] = []\n",
    "    dcd_results[\"test_accuracy\"] = []\n",
    "    \n",
    "    if not sparse:\n",
    "        mat = np.stack([bias[1].T] * bias[0].shape[1], axis = 1)    \n",
    "        Z = bias[0] * mat\n",
    "        biasQ = Z @ Z.T\n",
    "    \n",
    "        mat = np.stack([unbias[1].T] * unbias[0].shape[1], axis = 1)    \n",
    "        Z = unbias[0] * mat\n",
    "        unbiasQ = Z @ Z.T\n",
    "        \n",
    "        del mat\n",
    "        del Z\n",
    "    \n",
    "    for t in times:\n",
    "        gc.collect()\n",
    "        print(f\"Dataset: {dataset}\", f\"\\tDimension: {dimension}\", f\"Time: {t}\")\n",
    "        (smo_time, smo_problem) = trainingSMOtimed(bias[0], bias[1], C, SMO_step, t, sparse = sparse)\n",
    "\n",
    "        (w, b) = smo_problem.getParameters()\n",
    "        svm = SVM(w, b)\n",
    "        if not sparse:\n",
    "            smo_results[\"primal_loss\"].append(loss_svm(w, b, C, bias[0], bias[1]))\n",
    "            smo_results[\"dual_loss\"].append(dual_loss_svm(smo_problem.a, biasQ))\n",
    "        smo_results[\"times\"].append(smo_time)\n",
    "        smo_results[\"train_accuracy\"].append(calculate_accuracy(bias[1], bias[0], svm))\n",
    "        smo_results[\"test_accuracy\"].append(calculate_accuracy(bias_test[1], bias_test[0], svm))\n",
    "\n",
    "        (dcd_time, dcd_problem) = trainingDCDtimed(unbias[0], unbias[1], C, t)\n",
    "        \n",
    "        w = dcd_problem.getParameters()        \n",
    "        svm = SVM(w, 0)\n",
    "        if not sparse:\n",
    "            dcd_results[\"primal_loss\"].append(loss_svm(w, 0, C, unbias[0], unbias[1]))\n",
    "            dcd_results[\"dual_loss\"].append(dual_loss_svm(dcd_problem.a, unbiasQ))\n",
    "        dcd_results[\"times\"].append(dcd_time)\n",
    "        dcd_results[\"train_accuracy\"].append(calculate_accuracy(unbias[1], unbias[0], svm))\n",
    "        dcd_results[\"test_accuracy\"].append(calculate_accuracy(unbias_test[1], unbias_test[0], svm))\n",
    "    \n",
    "    smo_file = open(f\"Results/smo_timed_{dimension}_{C}.json\", \"w\")\n",
    "    smo_file.write(json.dumps(smo_results, indent = 2))\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_timed_{dimension}_{C}.json\", \"w\")\n",
    "    dcd_file.write(json.dumps(dcd_results, indent = 2))\n",
    "    dcd_file.close()\n",
    "    print(\"Test Finished\") \n",
    "    \n",
    "def dcd_timed_test(dataset, dimension, times, C, test_size):\n",
    "    smo_results = {}\n",
    "    dcd_results = {}\n",
    "    \n",
    "    smo_results[\"dimension\"] = []\n",
    "    dcd_results[\"dimension\"] = []\n",
    "    (bias, unbias) = load_dataset(f\"datasets/{dataset}\", dimension = dimension)\n",
    "    \n",
    "    (W, X, Y, Z) = train_test_split(unbias[0], unbias[1], test_size = test_size)\n",
    "    unbias = (W, Y)\n",
    "    unbias_test = (X, Z)\n",
    "    dcd_results[\"dimension\"] = unbias[0].shape[0]\n",
    "\n",
    "    dcd_results[\"times\"] = []\n",
    "    dcd_results[\"primal_loss\"] = []\n",
    "    dcd_results[\"dual_loss\"] = []\n",
    "    dcd_results[\"train_accuracy\"] = []\n",
    "    dcd_results[\"test_accuracy\"] = []\n",
    "    \n",
    "    \n",
    "    for t in times:\n",
    "        gc.collect()\n",
    "        print(f\"Dataset: {dataset}\", f\"\\tDimension: {dimension}\", f\"Time: {t}\")\n",
    "\n",
    "        (dcd_time, dcd_problem) = trainingDCDtimed(unbias[0], unbias[1], C, t)\n",
    "        \n",
    "        w = dcd_problem.getParameters()        \n",
    "        svm = SVM(w, 0)\n",
    "        \n",
    "        dcd_results[\"times\"].append(dcd_time)\n",
    "        dcd_results[\"train_accuracy\"].append(calculate_accuracy(unbias[1], unbias[0], svm))\n",
    "        dcd_results[\"test_accuracy\"].append(calculate_accuracy(unbias_test[1], unbias_test[0], svm))\n",
    "\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_timed_{dimension}_{C}.json\", \"w\")\n",
    "    dcd_file.write(json.dumps(dcd_results, indent = 2))\n",
    "    dcd_file.close()\n",
    "    print(\"Test Finished\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1ca95-c182-4822-a113-4dcc28a10bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0.5 * i for i in range(1, 21)]\n",
    "big_times = list(range(1, 21))\n",
    "dataset = \"covtype_ones\"\n",
    "test_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb5da1-bc40-46b9-be89-2caf8815975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timed_test(dataset, 12500, times, 1, test_size, sparse = True)\n",
    "timed_test(dataset, 15000, times, 1, test_size, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd8814-084c-47a0-a079-5f6a20dd9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd_timed_test(dataset, 580000, big_times, 1, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b7dce-c904-48ab-a10d-b94baf5a88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd_timed_test(dataset, 580000, big_times, 5, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce95dfc-ad8a-4663-a4f7-d02ea14fbe6c",
   "metadata": {},
   "source": [
    "### Results over Time constraint\n",
    "\n",
    "Here, simply, are correlated the accuracy obtained with the time given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5b660-f017-4577-be39-3798a8562240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_over_time(dimension, C, save_fig = None, train = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    smo_file = open(f\"Results/smo_timed_{dimension}_{C}.json\", \"r\")\n",
    "    smo_results = json.loads(smo_file.read())\n",
    "    smo_file.close()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_timed_{dimension}_{C}.json\", \"r\")\n",
    "    dcd_results = json.loads(dcd_file.read())\n",
    "    dcd_file.close()\n",
    "    \n",
    "    if train:\n",
    "        plt.plot(smo_results[\"times\"], smo_results[\"train_accuracy\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "        plt.plot(dcd_results[\"times\"], dcd_results[\"train_accuracy\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "    else:\n",
    "        plt.plot(smo_results[\"times\"], smo_results[\"test_accuracy\"], label = \"SMO\", marker='o', linestyle='dashed')\n",
    "        plt.plot(dcd_results[\"times\"], dcd_results[\"test_accuracy\"], label = \"DCD\", marker='o', linestyle='dashed')\n",
    "\n",
    "    if train:\n",
    "        plt.ylabel(\"Train Accuracy\")\n",
    "    else:\n",
    "        plt.ylabel(\"Test Accuracy\")\n",
    "    plt.xlabel(\"Time Taken [s]\")\n",
    "    plt.title(f\"Case with {smo_results[\"dimension\"]} elements, (C = {C})\")\n",
    "    \n",
    "    plt.legend()\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "    plt.show()\n",
    "def dcd_plot_over_time(dimension, C, save_fig = None, train = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    dcd_file = open(f\"Results/dcd_timed_{dimension}_{C}.json\", \"r\")\n",
    "    dcd_results = json.loads(dcd_file.read())\n",
    "    dcd_file.close()\n",
    "    \n",
    "    if train:\n",
    "        plt.plot(dcd_results[\"times\"], dcd_results[\"train_accuracy\"], label = \"DCD\", marker='o', linestyle='dashed', color = \"orange\")\n",
    "    else:\n",
    "        plt.plot(dcd_results[\"times\"], dcd_results[\"test_accuracy\"], label = \"DCD\", marker='o', linestyle='dashed', color = \"orange\")\n",
    "\n",
    "    if train:\n",
    "        plt.ylabel(\"Train Accuracy\")\n",
    "    else:\n",
    "        plt.ylabel(\"Test Accuracy\")\n",
    "    plt.xlabel(\"Time Taken [s]\")\n",
    "    plt.title(f\"Case with {dcd_results[\"dimension\"]} elements, (C = {C})\")\n",
    "    \n",
    "    plt.legend()\n",
    "    if save_fig is not None:\n",
    "        if not os.path.exists(\"Plots\"):\n",
    "            os.mkdir(\"Plots\")\n",
    "    \n",
    "        plt.savefig(f\"Plots/{save_fig}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_over_time(12500, 1, train = False, save_fig = \"time_1.png\")\n",
    "plot_over_time(15000, 1, train = False, save_fig = \"time_2.png\")\n",
    "dcd_plot_over_time(580000, 1, train = False, save_fig = \"time_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db460fe3-c409-42a8-bfc3-0825a681fdff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
